{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting Models to Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Distilling our data into a model\n",
    "We are often faced with the task of fitting a model to data that has been aquired (from experiment or computation). The usual situation is that we have more data points than are needed to determine the coefficients of the model - an **over-determined system** - and we seek some kind of 'best fit' to the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ax = b\n",
    "<img src=\"images/Axb.png\" width=\"500\"/>\n",
    "\n",
    "If our model is a linear combination of terms, we can express it compactly using the matrix equation $\\mathbf{Ax} = \\mathbf{b}$, where:\n",
    " - $\\mathbf{A}$ is the **model** matrix. Each row of A is a different data point, each column is a different model term. For example, if we seek a straight line fit to a series of points $(x,y)$ then the first column of A would be a vector of $1$s and the second would be a vector of $x$ values.\n",
    " - $\\mathbf{x}$, our solution vector, is the vector of **loadings** (coefficients) for each term in our model that best matches our data points.\n",
    " - $\\mathbf{b}$, our **outcomes** vector. This is made up of each $y$ value from our set of $(x,y)$ data points.\n",
    " \n",
    "In an over-determined system, we have more data points (rows of $\\mathbf{A}$) than we have coefficients (entries in $\\mathbf{x}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Solving a uniquely determined system using the inverse of A\n",
    "As an illustration, we start with two data points and we seek a line of best fit! We have two data points and two coefficients for our model, so we have a uniquely determined system. In this case, $\\mathbf{A}$ is a square matrix and we can solve $\\mathbf{Ax}=\\mathbf{b}$ (to find our coefficients vector, $\\mathbf{x}$) by inverting $\\mathbf{A}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "slope = 3.                             # Our data points lie on a line defined by slope,\n",
    "intercept = 2.                         # and intercept\n",
    "x_data = np.array([0,1])\n",
    "y_data = intercept + slope*x_data\n",
    "plt.scatter(x_data, y_data, c='r');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones([2,2])                     # Assemble the A matrix. Start by making all entries 1\n",
    "A[:,1] = x_data                        # then replace the second column with our independent variable, x_data\n",
    "coeffs = np.linalg.inv(A) @ y_data     # We call our solution vector x (in Ax=b) coeffs, and find it by inverting A\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = coeffs[0] + coeffs[1]*x_data   # Generate our model line using coeffs\n",
    "plt.scatter(x_data, y_data, c='r')\n",
    "plt.plot(x_data, y_fit);               # Plot the line to check the fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Solving an over-determined system using the pseudo-inverse of A\n",
    "We now construct the more common case where we have many more data points than coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 100                            # Number of data points\n",
    "x_data = np.linspace(0,1,npts)\n",
    "y_data = intercept + slope*x_in + 0.2*np.random.normal(size=npts)  # Add noise to the data\n",
    "plt.scatter(x_in,y_out, c='r', alpha=0.5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.ones([npts,2])                 # We constuct A in the same way as before - our model (linear fit) has not changed\n",
    "A[:,1] = x_data\n",
    "coeffs = np.linalg.pinv(A) @ y_data   # We now use the pseudo-inverse of A\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = coeffs[0] + coeffs[1]*x_data\n",
    "plt.scatter(x_data, y_data, c='r',alpha = 0.5)\n",
    "plt.plot(x_data, y_fit, 'b', lineWidth = 2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Relationship between the pseudo-inverse and the SVD\n",
    "If we obtain the SVD of $\\mathbf{A}$ we may write,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{b} &= \\mathbf{A}\\mathbf{x} \\\\\n",
    "&= \\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^T}\\:\\mathbf{x}\\quad\\text{,}\n",
    "\\end{align}\n",
    "\n",
    "and so\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{x} &= (\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^T})^{-1}\\:\\mathbf{b} \\\\\n",
    "&= \\mathbf{V}\\mathbf{\\Sigma^{-1}}\\mathbf{U^T}\\:\\mathbf{b}\\quad\\text{.}\n",
    "\\end{align}\n",
    "\n",
    "Since $\\mathbf{\\Sigma}$ is a diagonal matrix, the inverse is obtained by taking the reciprocal of all the non-zero diagonal entries.\n",
    "\n",
    "We can see that the 'pseudo-inverse' is $\\mathbf{V}\\mathbf{\\Sigma^{-1}}\\mathbf{U^T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,VT = np.linalg.svd(A)              # Obtain the SVD of A\n",
    "Sinv = np.zeros([2,npts])              # Construct the inverse of S by\n",
    "Sinv[:2,:2]=np.diag(np.reciprocal(S))  # taking the reciprocal of the entries on the diagonal \n",
    "Ainv = VT.T @ Sinv @ U.T               # Invert the SVD\n",
    "coeffs = Ainv @ y_data                 # Find the coeffs vector \n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. What is the pseudo-inverse doing?\n",
    "When we use the pseudo-inverse, we are obtaining a 'least squares' fit. Formally, this is written as,\n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x} = \\underset{x}{\\text{argmin}}\\: ||\\mathbf{Ax}-\\mathbf{b} ||_2\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\ell_2$ norm means,\n",
    "\n",
    "\\begin{equation}\n",
    "||\\mathbf{a}||_2 = \\sqrt{\\sum_{i=1}^n\\:a_i^2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Over-fitting and sparsity\n",
    "A property of the $\\ell_2$ norm is that it leads to coefficients in all of the model terms and this may not be a desirable property. \n",
    "\n",
    "As an example, we fit a higher order polynomial to our set of points: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npoly = 10                           # The order of the polynomial we are fitting\n",
    "A = np.zeros([npts, npoly])          # Form our model matrix, A\n",
    "for n in range(npoly):\n",
    "    A[:,n] = x_data**n               # Column n of A is our x_data vector raised to the power n\n",
    "coeffs = np.linalg.pinv(A) @ y_data  # Use the pseudo-inverse to obtain a least squares fit\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `coeffs` vector has non-zero values for all of the model terms. This is guaranteed to fit our data with minimal error in an $\\ell_2$ norm sense:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = np.zeros(npts)\n",
    "for n in range(npoly):\n",
    "    y_fit = y_fit + coeffs[n]*x_data**n    \n",
    "plt.scatter(x_data,y_data,c='r',alpha=0.5)\n",
    "plt.plot(x_data, y_fit, 'b', lineWidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But is likely to be unreliable if we extrapolate in $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.linspace(-0.2,1.2,npts)          # extend our x rang\n",
    "y_fit = np.zeros(npts)\n",
    "for n in range(npoly):\n",
    "    y_fit = y_fit + coeffs[n]*x_test**n\n",
    "plt.scatter(x_data,y_data,c='r',alpha=0.5)\n",
    "plt.plot(x_test, y_fit, 'b', lineWidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One way to overcome this is to find our model coefficients by solving a different optimisation problem, \n",
    "\n",
    "\\begin{equation}\n",
    "\\hat{x} = \\underset{x}{\\text{argmin}}\\: ||\\mathbf{Ax}-\\mathbf{b} ||_2 + \\alpha ||\\mathbf{x}||_1 \\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where the $\\ell_1$ norm means,\n",
    "\n",
    "\\begin{equation}\n",
    "||\\mathbf{a}||_1 = \\sum_{i=1}^n\\:|a_i|\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "What is happening now is that the optimisation is penalising solutions with large sums of loading coefficients. This type of fit will avoid coefficient vectors which have a non-zero value for each model term. This is called the **Lasso** fit and is already implemented in the scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoreg = Lasso(alpha=0.007, max_iter=1e5)  # set up a Lasso regression\n",
    "lassoreg.fit(A,y_data)                       # apply it to our model matrix A, and outcomes vector, y_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs=np.copy(lassoreg.coef_)               # form our coeffs vector\n",
    "coeffs[0]=lassoreg.intercept_\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso regression promotes a *sparse* set of model coefficients.\n",
    "\n",
    "We can see how our model fits our data, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fit = np.zeros(npts)\n",
    "for n in range(npoly):\n",
    "    y_fit = y_fit + coeffs[n]*x_data**n\n",
    "plt.scatter(x_data, y_data, c='r', alpha=0.5)\n",
    "plt.plot(x_data, y_fit, 'b', lineWidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and also how it can be used to extrapolate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test=np.linspace(-0.2,1.2,npts)\n",
    "y_fit = np.zeros(npts)\n",
    "for n in range(npoly):\n",
    "    y_fit = y_fit + coeffs[n]*x_test**n\n",
    "plt.scatter(x_data, y_data, c='r', alpha=0.5)\n",
    "plt.plot(x_test, y_fit, 'b', lineWidth=2);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the Lasso regression, we must choose a value of $\\alpha$. What happens to the fit as we change $\\alpha$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. A more realistic example\n",
    "Imagine we measured the loss coefficient $Y_p$ of a blade at a range of incidences $i$. We would obtain a 'loss bucket', perhaps subject to some measurement noise: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts=100\n",
    "i=np.linspace(-10,10,npts)\n",
    "Yp = 0.01 + i*i/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yp = Yp * (1+ 0.1*np.random.normal(size=npts))\n",
    "plt.scatter(i, Yp, c='r', alpha=0.5)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('Yp');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have set $Y_p$ to be a quadratic function of $i$. But if we didn't know this beforehand, we may also be trying to fit $Y_p$ to some other variables that we had measured. We simulate this be adding two columns to our model matrix $\\mathbf{A}$ that contain random numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = 0.01*np.random.normal(size=[npts,5])   # First fill A with random numbers\n",
    "A[:,0] = 1.                                # Then overwrite first 3 columns with 1, i and i*i\n",
    "A[:,1] = i\n",
    "A[:,2] = i*i\n",
    "coeffs = np.linalg.pinv(A) @ Yp            # We use the pseudo-inverse to perform an l2 regression\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `coeffs` vector contains loadings of all the model terms, even the random noise.\n",
    "\n",
    "It will still fit our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = A @ coeffs                           # If we are applying our model to the independent variables in the data used to form the model, we can use this matrix multiply to obtain the fit\n",
    "plt.scatter(i,Yp,c='r',alpha=0.5)\n",
    "plt.plot(i,fit,'b',linewidth=2)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('Yp');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try with Lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lassoreg = Lasso(alpha=0.0003, normalize=True, max_iter=1e5)\n",
    "lassoreg.fit(A,Yp)\n",
    "coeffs=np.copy(lassoreg.coef_)\n",
    "coeffs[0]=lassoreg.intercept_\n",
    "coeffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Lasso regression promotes a sparse set of coefficients (try varying $\\alpha$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = A @ coeffs\n",
    "plt.scatter(i,Yp,c='r',alpha=0.5)\n",
    "plt.plot(i,fit,'b',linewidth=2)\n",
    "plt.xlabel('i')\n",
    "plt.ylabel('Yp');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
