{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "\n",
    "In recent years, neural networks have grown remarkably in their variations and range of application. This has been driven by increases in both the available training data and also in compute power.\n",
    "\n",
    "Neural networks can interpolate high dimensional data in a flexible, adaptable way. Although each 'neuron' typically uses a simple (non-linear) function to map input to output, the multiple neurons in a layer, and then the multiple layers in a network, give neural networks enormous scope."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multi-layer neural networks\n",
    "<img src=\"images/multi-layer.png\" width=800>\n",
    "\n",
    "A multi-layered neural network consists of an input layer $\\mathbf{x}$, and output layer $\\mathbf{y}$ and one or more hidden layers, $\\mathbf{z_i}$. The neurons in each layer can be connected to some or all of the data available at the previous layer. Each neuron applies an *activation function* to a weighted sum of the inputs. As an example, the second hidden layer $\\mathbf{z_2}$ is related to the first hidden layer $\\mathbf{z_1}$, \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{z_2} = f (\\mathbf{A}, \\mathbf{z_1})\\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where $f$ is the activation function and $\\mathbf{A}$ is a matrix of *weights* and *biases*. For a particular neuron, the output will be given by,\n",
    "\n",
    "\\begin{equation}\n",
    "z_{2j} = f\\Biggl( \\sum_i w_i z_{1i} + b_j \\Biggr)\\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where the summation is over the $i$ input values of $\\mathbf{z_1}$, $w_i$ is the weighting applied to the $i$th input, and $b_j$ is the bias. In training the neural network, we seek optimum values for all the weight and biases (the *parameters*) of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Activation functions\n",
    "\n",
    "The activation function ($f$ in the above equation) is usually a simple, differentiable, non-linear function. Common choices are the tanh function and the so-called rectified linear unit (ReLU) which is linear for a positive input, and zero otherwise.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python to plot some common activation functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch                                        # we will use pytorch to build our neural network\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5,5,100)\n",
    "y1 = x                   # linear \n",
    "y2 = 1./(1+np.exp(-x))   # logistic\n",
    "y3 = np.tanh(x)          # tanh\n",
    "y4 = np.clip(x,0,None)   # rectified linear unit (ReLU)\n",
    "\n",
    "plt.figure(figsize=(20,3))\n",
    "plt.subplot(1,4,1)\n",
    "plt.plot(x,y1)\n",
    "plt.title('Linear')\n",
    "plt.subplot(1,4,2)\n",
    "plt.plot(x,y2)\n",
    "plt.title('Logistic')\n",
    "plt.subplot(1,4,3)\n",
    "plt.plot(x,y3)\n",
    "plt.title('Tanh')\n",
    "plt.subplot(1,4,4)\n",
    "plt.plot(x,y4)\n",
    "plt.title('ReLU');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training is an optimisation problem\n",
    "In training the network, we seek to minimise the error $E$ by changing the parameters in our matrices $\\mathbf{A_j}$. If we define the error as a sum of the squares, we could write this as\n",
    "\n",
    "\\begin{equation}\n",
    "\\underset{\\mathbf{A_j}}{\\text{argmin}}\\: E(\\mathbf{A_1}, \\mathbf{A_2},\\dots, \\mathbf{A_M}) = \\underset{\\mathbf{A_j}}{\\text{argmin}}\\: \\sum_{k=1}^n \\Bigl( f(\\mathbf{x_k}, \\mathbf{A_1}, \\mathbf{A_2},\\dots, \\mathbf{A_M} ) - \\mathbf{y_k} \\Bigr)^2\n",
    "\\end{equation}\n",
    "\n",
    "where $f$ is the total input-to-output function created by the network and $\\mathbf{y_k}$ is the truth.\n",
    "\n",
    "To see how this optimisation can be done, we now consider a simple one node, one hidden layer network.\n",
    "\n",
    "<img src=\"images/backprop.png\" width=500>\n",
    "\n",
    "When the error $E$ is minimised with respect to our network parameters ($a$ and $b$ in this case), we will have,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial a} = \\frac{\\partial E}{\\partial b} = 0\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "If we write,\n",
    "\n",
    "\\begin{equation}\n",
    "E = \\frac{1}{2}(y_0-y)^2\\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where $y$ is the output of the network and $y_0$ is the truth, then, by the chain rule,\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial E}{\\partial a} = -(y_0 - y)\\frac{dy}{dz}\\frac{dz}{da}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "The derivatives $dy/dz$ and $dz/da$ can be computed from the activation functions $f$ and $g$. Using the chain rule, we can see that the derivatives we require move upstream in the network, from output to input. This is called **back propagation** and is a key concept in neural networks. \n",
    "\n",
    "Once we have the gradient in error with respect to a particular weight, we may update that weight using,\n",
    "\n",
    "\\begin{equation}\n",
    "a_{\\text{new}} = a_{\\text{old}} + \\delta\\frac{\\partial E}{\\partial a}\\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\delta$ is the *learning rate*. \n",
    "\n",
    "As the number of trainable parameters in our matrices $\\mathbf{A_j}$, and the number of data points $n$, increases, using this *gradient descent* approach to update the parameters becomes intractable. **Stochastic gradient descent** uses a random subset, or *batch*, of the total $n$ points that are available. Having selected a batch, the errors and gradients (by back propagation) are computed based on the results of applying the network *to that batch*. The network parameters are then updated, a new batch selected, and the process repeats. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Using Pytorch for neural networks\n",
    "\n",
    "Pytorch is a widely used framework for developing Machine Learning models in Python. We will use it to develop a multi-layer neural network (2 hidden layers) to connect one output variable to six input variables.\n",
    "\n",
    "We first create a `class` called `Net` which initialises the arrays (in the `__init__` function) and defines the structure of the neural network (the `forward` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):                         # Define \"Net\" as a subclass of the torch.nn module\n",
    "    \n",
    "  def __init__(self):                               # When we create an instance of Net, the _init_ function is called\n",
    "    super(Net, self).__init__()                     # This line means that when we create a Net and run _init_ , we also run the _init_ of the torch.nn Class\n",
    "    self.hid1 = torch.nn.Linear(6, 10)              # Set the size of hidden layer 1: 6 inputs x 10 outputs\n",
    "    self.hid2 = torch.nn.Linear(10, 10)             # Set the size of hidden layer 2: 10 inputs x 10 outputs\n",
    "    self.oupt = torch.nn.Linear(10, 1)              # The output layer is of size: 10 inputs x 1 output\n",
    "    torch.nn.init.xavier_uniform_(self.hid1.weight) # initialise the weights (Xavier uniform distribution)\n",
    "    torch.nn.init.zeros_(self.hid1.bias)            # initialise bias (zero)\n",
    "    torch.nn.init.xavier_uniform_(self.hid2.weight)\n",
    "    torch.nn.init.zeros_(self.hid2.bias)\n",
    "    torch.nn.init.xavier_uniform_(self.oupt.weight)\n",
    "    torch.nn.init.zeros_(self.oupt.bias)\n",
    "    \n",
    "  def forward(self, x):                             # Now define the NN:\n",
    "    z1 = torch.tanh(self.hid1(x))                   # z1 is output from the tanh activation function applied to the result of hid1 on x\n",
    "    z2 = torch.tanh(self.hid2(z1))                  # z2 is tanh function applied to result of hid2 on z1\n",
    "    y  = self.oupt(z2)                              # y is result of oupt on z2\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`normalise` is a helper function to make the columns of an array go from 0 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalise(dataarray):\n",
    "  dataarraynormalise = np.zeros_like(dataarray)\n",
    "  for j,column in enumerate(dataarray.T):\n",
    "    minvalue = np.min(column)\n",
    "    maxvalue = np.max(column)\n",
    "    print('min',minvalue,'max',maxvalue)\n",
    "    dataarraynormalise[:,j] = np.divide(np.subtract(dataarray[:,j],minvalue),maxvalue - minvalue)\n",
    "  return(dataarraynormalise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loaddata` is a function to load a csv file and splits into inputs and outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loaddata(filename):\n",
    "  train_file = np.loadtxt(filename, delimiter = \",\",skiprows = 1)  # load data (skip first line as this is headings)\n",
    "  test_file = np.loadtxt(filename, delimiter = \",\",skiprows = 1)   # in this case we use the same data file for our test and training data\n",
    "  print(\"Loading  data into memory \")\n",
    "  columns_train_x = [0,1,2,3,4,5]                                  # these columns are inputs\n",
    "  columns_train_y = [6]                                            # this column is the output\n",
    "  test_x = np.array(test_file[:,columns_train_x])\n",
    "  test_y = np.array(test_file[:,columns_train_y])\n",
    "  train_x = np.array(train_file[:,columns_train_x])\n",
    "  train_y = np.array(train_file[:,columns_train_y])\n",
    "  return(train_x,train_y,test_x,test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1);  np.random.seed(1)                              # to make results repeatable, we make sure that the random number generators in pytorch and numpy are set to the same seed before we train the NN\n",
    "\n",
    "train_x,train_y,test_x,test_y = loaddata('data/exampledatasetNN.csv') # load the data file\n",
    "train_x = normalise(train_x)                                          # normalise the training input data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create out network `net` and set the error function and optimiser method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()                                             # net is an instance of our new nn Class, \"Net\"\n",
    "net = net.train()                                       # set the mode of net to be \"train\"\n",
    "bat_size = 50                                           # this is our batch size. We will load 50 rows of trainig data at a time.\n",
    "loss_func = torch.nn.MSELoss()                          # mean squared error\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.01) # use the built in Adam optimiser. This is a related to Stochastic Gradient Descent, but adapts the learning rate for each trainable parameter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_items = len(train_x)                                  # total number of rows in training set\n",
    "batches_per_epoch = n_items // bat_size                 # number of batches in training set ( // rounds down to nearest integer)\n",
    "max_batches = 1000 * batches_per_epoch\n",
    "\n",
    "print(\"Starting training\")\n",
    "\n",
    "for b in range(max_batches):\n",
    "    curr_bat = np.random.choice(n_items, bat_size,replace=False) # current batch is a random sample of row indices\n",
    "    X = torch.Tensor(train_x[curr_bat])                          # the current inputs X\n",
    "    Y = torch.Tensor(train_y[curr_bat]).view(bat_size,1)         # the current true outputs (reshaped) Y\n",
    "    optimizer.zero_grad()                                        # set the optimiser gradients to zero\n",
    "    oupt = net(X)                                                # run our net on X and obtain oupt\n",
    "    loss_obj = loss_func(oupt, Y)                                # compute the loss objective based on current oupt and truth (Y)\n",
    "    loss_obj.backward()                                          # compute the gradients of the loss objective with respect to all the trainable parameters in our NN using back propagation\n",
    "    optimizer.step()                                             # update the trainable parameters\n",
    "    print(b,loss_obj)\n",
    "    \n",
    "print(\"Training complete \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assess our model by plotting the actual $y$ against the predicted $y$ from the network. We also compute the coefficient of determinaton ($R^2$ score). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = normalise(test_x)\n",
    "X = torch.Tensor(test_x)\n",
    "y = net(X)\n",
    "y = y.detach().numpy()   # convert output from torch tensor to numpy array for plotting\n",
    "plt.scatter(test_y, y, color='black',s=1)\n",
    "plt.xlabel(\"actual\")\n",
    "plt.ylabel(\"predicted\")\n",
    "print(\"R^2: \" + str(r2_score(test_y, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Cross-validation\n",
    "We mention here two challenges associated with neural networks. \n",
    "\n",
    "The first is interpretability; although we can quantify how well a network fits the data, it is not easy to assess how or why the fit has been achieved. \n",
    "\n",
    "The second is over-fitting; we must always check that the network we have built is reliable and has not been over-fitted to our particular training sample. Note that, in the above example, the test set is the same as the training set and this is *not good practice*. A better approach is to reserve a portion of the data (20 percent is typical) for testing only (i.e. this data is not used in training). In **cross-validation**, the model is built $k$ times using random sample sets of the training data. The parameters of the model are then set to the average of the parameters found in the $k$ models. A new model, with these averaged parameters, is then tested on the witheld portion of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
