{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Singular Value Decomposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Using a matrix to store our data\n",
    "We imagine that the data we are interested in are shaped into a $n\\times m$ matrix, $\\mathbf{X}$. Most of our data will be real numbers only, so $\\mathbf{X}\\in\\mathbb{R}^{n\\times m}$.\n",
    "\n",
    "The data in $\\mathbf{X}$ might be spatially 2-D (a single image, for example) or it might be composed such that each column is data from a set of spatial postions at different instants in time,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix} \n",
    "\\mid & \\mid &  & \\mid  \\\\ \n",
    "\\mathbf{x_1} & \\mathbf{x_2} & \\cdots  & \\mathbf{x_m} \\\\\n",
    "\\mid & \\mid &  & \\mid \\end{bmatrix}\\quad\\text{,}\n",
    "\\end{equation}\n",
    "\n",
    "where each column $\\mathbf{x_k}$ is data from a set of $n$ sensors at a specific instant in time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Definition of the Singular Value Decomposition\n",
    "\n",
    "Any matrix $\\mathbf{X}$ can be written as the product of 3 special matrices, $\\mathbf{U}$, $\\mathbf{\\Sigma}$ and $\\mathbf{V^T}$ (where $\\mathbf{V^T}$ is the transpose of $\\mathbf{V}$),\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U}\\,\\mathbf{\\Sigma}\\,\\mathbf{V^T}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "$\\mathbf{U}$ and $\\mathbf{V}$ are both *orthogonal* matrices because,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{U}\\mathbf{U^T} &=\\mathbf{U^T}\\mathbf{U}=\\mathbf{I}\\quad\\text{and} \\\\\n",
    "\\mathbf{V}\\mathbf{V^T} &=\\mathbf{V^T}\\mathbf{V}=\\mathbf{I}\\quad\\text{.}\n",
    "\\end{align}\n",
    "\n",
    "The $\\mathbf{\\Sigma}$ matrix only has non-zero entries on the diagonal. These are the **singular values** and are arranged in descending order.\n",
    "\n",
    "(If our data contains complex numbers, $\\mathbf{X}\\in\\mathbb{C}^{n\\times m}$, then the SVD still works but now $\\mathbf{U}$ and $\\mathbf{V}$ are *unitary* matrices so that $\\mathbf{U}\\mathbf{U^*}=\\mathbf{I}$ where $^*$ denotes conjugate transponse.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Shapes of the matrices in the SVD\n",
    "\n",
    "If $\\mathbf{X}\\in\\mathbb{R}^{n\\times m}$ then $\\mathbf{U}\\in\\mathbb{R}^{n\\times n}$, $\\mathbf{\\Sigma}\\in\\mathbb{R}^{n\\times m}$ and $\\mathbf{V^T}\\in\\mathbb{R}^{m\\times m}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/svd.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how we can evaluate the SVD using Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "X = np.random.rand(n,m)     # Create a matrix of random numbers (between 0. and 1.) with n rows and m columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X                           # In a notebook, this writes X to the screen - similar to print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,VT = np.linalg.svd(X)   # Use numpy's linalg.svd to evaluate the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U.shape, S.shape, VT.shape  # Find the shapes of U, S and VT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expected $\\mathbf{U}\\in\\mathbb{R}^{n\\times n}$, $\\mathbf{\\Sigma}\\in\\mathbb{R}^{n\\times m}$ and $\\mathbf{V^T}\\in\\mathbb{R}^{m\\times m}$, but `numpy.linalg.svd` has returned $\\mathbf{\\Sigma}$ as a vector of length $n$. `numpy` has returned a compact description of $\\mathbf{\\Sigma}$, noting that it is a diagonal matrix and, since $n>m$ in our case, the bottom $n-m$ rows of $\\mathbf{\\Sigma}$ are zeros.\n",
    "\n",
    "If we need to, we can reconstruct the full $\\mathbf{\\Sigma}$ using:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sfull = np.zeros([n,m])     # Make a matrix (array) that is all zeros and of the correct shape\n",
    "Sfull[:m,:m] = np.diag(S)   # Insert the diagonal matrix made of the vector S. \n",
    "                            # The [:m,:m] notation means the first m rows and first m columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sfull                       # display the full Sigma matrix (note the order of the singular values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can check the SVD by reconstructing X:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xrecon = U @ Sfull @ VT     # @ is matrix multiplication in Python 3\n",
    "Xrecon "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Properties of U and V\n",
    "\n",
    "We can also demonstrate some properties of $\\mathbf{U}$ (and $\\mathbf{V}$). The vectors that form the columns of $\\mathbf{U}$ (and the columns of $\\mathbf{V}$) have unit magnitude and are orthogonal to the other columns of the matrix, i.e. they are orthonomal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[:,0] @ U[:,0].T           # Check that the magnitude of the first column of U = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U[:,0] @ U[:,1].T           # Check that the first column is orthogonal to the second column"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Key intepretation of $\\mathbf{U}$ and $\\mathbf{V}$ (from Brunton and Kutz):\n",
    "\n",
    "* Columns of $\\mathbf{U}$ are an orthonormal basis for the column space of $\\mathbf{X}$ \n",
    "* Columns of $\\mathbf{V}$ (not $\\mathbf{V^T}$ are an orthonormal basis for the row space of $\\mathbf{X}$ \n",
    "\n",
    "If each column of $\\mathbf{X}$ contains spatial data at a different time instant, then $\\mathbf{U}$ relates to spatial patterns, and $\\mathbf{V}$ to tempoeral patterns.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Matrix appoximation using the SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD identifies the *rank* of a matrix by the number of singular values. The rank is the maximum number of linearly independent columns (or rows) of the matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 3\n",
    "n = 5\n",
    "X = np.random.rand(n,m)      # Make a new (n,m) matrix of random numbers\n",
    "U,S,VT = np.linalg.svd(X)    # Compute the SVD\n",
    "S                            # Show the singular values (since X is made of random numbers, we expect all columns (and rows)\n",
    "                             # to be independent and so X will be of rank m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[:,2] = X[:,1]*2.           # Change the third column (indices start from 0 in Python) to be twice the second column\n",
    "U,S,VT = np.linalg.svd(X)    # Now when we compute the SVD, we expect two singular values because the third column is not idependent of the second\n",
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can approximate $\\mathbf{X}$ in a lower rank using the SVD. The optimal rank $r$ approximation is given by the largest $r$ singular values. A nice way to show this is by using an image as the data matrix $\\mathbf{X}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = np.load('data/img.npy')\n",
    "plt.imshow(img, cmap = 'gray', vmin = 0, vmax = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U,S,VT = np.linalg.svd(img)     # Perform the SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S.size                          # Tells us the rank of the img matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now approximate the `img` matrix using a new matrix of rank $r$.\n",
    "\n",
    "![](images/svd-r.png)\n",
    "\n",
    "The SVD now contains only the leading $r$ terms of $\\mathbf{\\Sigma}$ (the largest $r$ singular values), $\\mathbf{\\hat{U}}$ is the first $r$ columns of $\\mathbf{U}$ and $\\mathbf{\\hat{V}^T}$ is the first $r$ rows of $\\mathbf{V^T}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 50                                                 # Rank of new matrix\n",
    "imgRecon = U[:,:r] @ np.diag(S[:r]) @ VT[:r,:]         # Use first r columns of U, first r values in S, and first r rows in VT\n",
    "plt.imshow(imgRecon, cmap='gray', vmin = 0, vmax = 1)\n",
    "plt.title(\"Approximation; Rank = \" + str(r) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This represents a signficant reduction in the amount of data needed to describe the image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "size_orig = img.flatten().size\n",
    "size_approx = U[:,:r].flatten().size + S[:r].flatten().size + VT[:r,:].flatten().size\n",
    "size_approx / size_orig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even a rank 10 approximation is surprisingly clear: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = 10 \n",
    "imgRecon = U[:,:r] @ np.diag(S[:r]) @ VT[:r,:]\n",
    "plt.imshow(imgRecon, cmap='gray')\n",
    "plt.title(\"Approximation; Rank = \" + str(r) ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The matrix formed when a single column of $\\mathbf{U}$ and a single row of $\\mathbf{V^T}$ are multiplied together is of rank 1. Our appoximation is the superposition of $r$ of these rank 1 matrices, each multiplied by a their respective singular value. \n",
    "\n",
    "We can inspect the individual rank 1 matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=plt.figure(figsize=(10,5))\n",
    "for k in range(10):\n",
    "    plt.subplot(2,5,k+1)\n",
    "    plt.title('k = ' + str(k))\n",
    "    rank1matrix = np.outer(U[:,k] , VT[k,:])   # Form the rank 1 matrix\n",
    "    plt.imshow(rank1matrix, cmap='gray')\n",
    "    plt.axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The approximation of a matrix $\\mathbf{X}$ up to rank $r$ is often given as an example of data compression. In engineering, we take a slightly different perspective. If a reduced rank matrix is a good approximation to the original data, then this tells us about the underlying low rank structure of our data. It is often this low rank structure engineers can apply from one example of the problem to the next; it is both interpretable and transferrable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SVD is useful in a wide range of data science applications. In the next notebook, we will look at one of the most common, Principal Component Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
