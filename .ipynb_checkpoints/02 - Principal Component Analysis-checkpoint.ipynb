{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The goal - dimensionality reduction\n",
    "\n",
    "Principal Component Analysis (sometimes called Proper Orthogonal Decomposition) is a technique for reducing the dimensions of data. \n",
    "\n",
    "If we think of our data as points in a coordinate system (easy with 2-D data, but harder to visualise in n-D), what PCA does is project the data onto a new set of axes, where the basis vectors for the new axes are a **linear combination** of the original coordinate system basis vectors. \n",
    "\n",
    "The key to PCA is that the new basis vectors are chosen in a defined way: the basis vector of the first principle component is aligned so as to capture the maximum possible variation in the data; the second principle component captures the second largest variation in the data; and so on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/pca.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above is a cartoon illustration. The data points have a large variation in both the x and y basis vectors; note, also, that 2 data points that differ in x will also differ in y - these changes are correlated. The principal components (right picture) are chosen so that the maximum variation in the data is obtained along the PC1 basis vector; also, there is no correlation between a change in PC2 and in PC1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Link to the Singular Value Decomposition\n",
    "Principal Component Analyis and the Singular Value Decomposition are closely related. The easiest way to illustrate this is by assuming the answer: for mean-centred data (each dimension has zero mean), the basis vectors required for PCA are the columns of $\\mathbf{U}$ in the SVD.\n",
    "\n",
    "Our data is stored in matrix $\\mathbf{X}$,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\begin{bmatrix} \n",
    "\\mid & \\mid &  & \\mid  \\\\ \n",
    "\\mathbf{x_1} & \\mathbf{x_2} & \\cdots  & \\mathbf{x_m} \\\\\n",
    "\\mid & \\mid &  & \\mid \\end{bmatrix}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "Each column $\\mathbf{x_k}$ is a new data point, and each row is a different dimension. For the 2-D example above, $\\mathbf{X}$ would have 2 rows and as many columns as we have points.\n",
    "\n",
    "We perform the SVD on $\\mathbf{X}$, so that,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{X} = \\mathbf{U}\\,\\mathbf{\\Sigma}\\,\\mathbf{V^T}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "Now, we recall that $\\mathbf{U}$ (and also $\\mathbf{V}$) is orthonormal: each column is a vector of unit magnitude, and all the column vectors are orthogonal. If we want to project our original data $\\mathbf{X}$ on to the basis vectors formed by the columns of $\\mathbf{U}$ (to obtain a new view of our data which we will call $\\mathbf{Y}$) then all we have to do is,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{Y}=\\mathbf{U^T}\\mathbf{X}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "To make further progress, we introduce *variance* and *covariance*. If we have two samples of data, each with zero mean, stored in vector $\\mathbf{a}$ and a vector $\\mathbf{b}$, then the variances are:\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_\\mathbf{a} = \\frac{1}{n-1}\\mathbf{a}\\mathbf{a^T} \\\\\n",
    "\\sigma^2_\\mathbf{b} = \\frac{1}{n-1}\\mathbf{b}\\mathbf{b^T}\n",
    "\\end{equation}\n",
    "\n",
    "(the n-1 is because we treat the vectors as a sample of data, and we seek an 'unbiased estimator').\n",
    "\n",
    "The covariance of $\\mathbf{a}$ and $\\mathbf{b}$ is\n",
    "\n",
    "\\begin{equation}\n",
    "\\sigma^2_\\mathbf{ab} = \\frac{1}{n-1}\\mathbf{a}\\mathbf{b^T}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "If the data in $\\mathbf{a}$ and $\\mathbf{b}$ are correlated, the covariance will be high. If $\\mathbf{a}$ and $\\mathbf{b}$ are not correlated, the covariance will be low.\n",
    "\n",
    "We can obtain the covariance of our transformed data matrix $\\mathbf{Y}$ as follows,\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathbf{C_Y}=\\frac{1}{n-1}\\mathbf{Y}\\mathbf{Y^T}\\quad\\text{.}\n",
    "\\end{equation}\n",
    "\n",
    "What do we want the covariance matrix, $\\mathbf{C_Y}$, to look like? The entries on the diagonal will be the variances in each of our (transformed) measurement dimensions. If we have chosen these dimensions correctly, i.e. we are using the principal components of the data, then all of the off-diagonal entries will be zero (uncorrelated).\n",
    "\n",
    "Since $\\mathbf{Y} = \\mathbf{U}^T\\mathbf{X}$,\n",
    "\n",
    "\\begin{align}\n",
    "\\mathbf{C_Y} &= \\frac{1}{n-1}\\mathbf{Y}\\mathbf{Y^T} \\\\\n",
    " &= \\frac{1}{n-1} (\\mathbf{U}^T\\mathbf{X})(\\mathbf{U}^T\\mathbf{X})^T \\\\\n",
    " &= \\frac{1}{n-1} \\mathbf{U}^T\\:\\mathbf{X}\\mathbf{X}^T\\:\\mathbf{U} \\\\\n",
    " &= \\frac{1}{n-1} \\mathbf{U}^T\\:\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^T}(\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^T})^T\\:\\mathbf{U} \\\\\n",
    " &= \\frac{1}{n-1} \\mathbf{U}^T\\:\\mathbf{U}\\mathbf{\\Sigma}\\mathbf{V^T}\\mathbf{V}\\mathbf{\\Sigma}\\mathbf{U^T}\\:\\mathbf{U} \\\\\n",
    " & = \\frac{1}{n-1} \\mathbf{\\Sigma}^2\n",
    "\\end{align}\n",
    "\n",
    "This is exactly what we want. $\\mathbf{C_Y}$ is a matrix which only has entries on the diagonal. This means that the basis vectors on to which we have projected $\\mathbf{X}$ must be the principal components. We also know that $\\mathbf{\\Sigma}$ has the singular values in order largest to smallest, so the columns of $\\mathbf{U}$ are our principal component basis vectors arranged such that the first column gives the basis with the largest variance in $\\mathbf{Y}$ and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Simple demonstration of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use Python to demonstrate PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we construct an artifical set of data like the cartoon above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "npts = 500                                        # number of points\n",
    "angle = 20.                                       # this is the angle of our first principal component direction\n",
    "angle_rad = angle/180.*np.pi                 \n",
    "s = np.random.normal(size=npts)                   # the s and n coordinates are along, and perpendicular, to PC1\n",
    "n = 0.1* np.random.normal(size=npts)\n",
    "x = s*np.cos(angle_rad) - n*np.sin(angle_rad)     # obtain the x and y of each point\n",
    "y = s*np.sin(angle_rad) + n*np.cos(angle_rad)\n",
    "x = x - np.mean(x)                                # make sure x and y have zero mean\n",
    "y = y - np.mean(y)\n",
    "plt.scatter(x,y,alpha=0.4)\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct our data matrix $\\mathbf{X}$. The matrix will have two rows (corresponding to x and y) and the number of columns will be the number of data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack((x,y))                                # numpy has a handy function for combinine two row vectors into a matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[0,:],X[1,:],alpha=0.4)\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtain the SVD of $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = np.linalg.svd(X)                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can immediately check to see if the first principle component is at the expected angle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.arctan(U[0,1]/U[0,0])/np.pi*180.             # U[0,1] is the y component of the first PC, U[0,0] is the x component"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now obtain $\\mathbf{Y}$ by projecting our data matrix on to the column vectors of $\\mathbf{U}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U.T @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can plot the data in this transformed basis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(Y[0,:],Y[1,:],alpha=0.4)\n",
    "plt.axis(\"equal\")\n",
    "plt.xlabel(\"PC 1\")\n",
    "plt.ylabel(\"PC 2\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. 'Eigenwakes'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demonstrations of PCA in the literature sometimes use an example with many photographs of faces. The principal component vectors (each of which is an image) are called [Eigenfaces](https://en.wikipedia.org/wiki/Eigenface). In the spirit of this example, we look at Eigenwakes!\n",
    "\n",
    "First load a set of wakes 1188 wakes from CFD simulations of compressor stators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wakes = np.load(\"data/cfd_images.npy\")\n",
    "wakes = wakes.T                               # take the transpose so that each column of wakes is an individual wake\n",
    "imsize = [56,56]                              # each wake is a 56 x 56 \"image\"\n",
    "n_wakes = wakes.shape[1]\n",
    "wakes.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first column of wakes is a clean wake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwake = 0\n",
    "wake = np.reshape(wakes[:,nwake],imsize)\n",
    "plt.imshow(wake, vmin=0, vmax=1.)\n",
    "plt.colorbar()\n",
    "plt.title(\"wake \"+str(nwake));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wake number 700, for example, has a large corner separation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwake = 700\n",
    "wake = np.reshape(wakes[:,nwake],imsize)\n",
    "plt.imshow(wake, vmin=0, vmax=1.)\n",
    "plt.colorbar()\n",
    "plt.title(\"wake \"+str(nwake));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To perform PCA, we need the mean of each row of our data matrix to be zero.\n",
    "Obtain the mean wake:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wake_mean = np.mean(wakes, axis = 1)\n",
    "plt.imshow(np.reshape(wake_mean,imsize), vmin=0, vmax=1.)\n",
    "plt.colorbar()\n",
    "plt.title(\"mean wake\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subtract the mean wake from each individual wake to obtain our data matrix, $\\mathbf{X}$. Then perform the SVD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = wakes - np.outer(wake_mean, np.ones([1,n_wakes]))\n",
    "U, S, VT = np.linalg.svd(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the columns of $\\mathbf{U}$ - our principal component vectors - as images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "for i in range(4):\n",
    "    plt.subplot(1,4,i+1)\n",
    "    plt.imshow(np.reshape(U[:,i],imsize))\n",
    "    plt.title('PC mode '+str(i+1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project $\\mathbf{X}$ on to the columns of $\\mathbf{U}$ to obtain $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = U.T @ X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do the wakes look like if we reduce the number of dimensions to `nPC = 10` ? Remember that the number of dimensions of our original data was 56 x 56 = 3136."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nPC = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reconstruct wake 0 using only the first nPC Principal components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwake = 0\n",
    "wake_recon = wake_mean + U[:,:nPC] @ Y[:nPC, nwake]\n",
    "plt.imshow(np.reshape(wake_recon,imsize), vmin=0, vmax=1.)\n",
    "plt.title(\"Wake \"+str(nwake)+\", reconstructed with \"+str(nPCA)+\" PC modes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and wake 700:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nwake = 700\n",
    "wake_recon = wake_mean + U[:,:nPC] @ Y[:nPC, nwake]\n",
    "plt.imshow(np.reshape(wake_recon,imsize), vmin=0, vmax=1.)\n",
    "plt.title(\"Wwake \"+str(nwake)+\", reconstructed with \"+str(nPCA)+\" PC modes\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the principal components (the values in $\\mathbf{Y}$) that are needed to reconstruct the wakes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx = np.arange(1, nPC+1)\n",
    "plt.scatter(xx, Y[:nPC, 0],c='b')\n",
    "plt.scatter(xx, Y[:nPC, 700],c='r')\n",
    "plt.xlabel('PC number')\n",
    "plt.ylabel('Value of PC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above figure, the blue points are from wake 0 - a clean wake. The red points are from wake 700 - a corner separation.\n",
    "\n",
    "Using a different technique (convolutional neural network image processing) all of the wakes in our data set have been labeled, and the results are in the cornsep.txt file. A value of 0 means the wake is clean, a value of 1 means a corner separation has been detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cornsep = np.loadtxt(\"data/cornsep.txt\", dtype=int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using these labels, we can plot the values of the principal components for all the wakes in $\\mathbf{Y}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_wakes):\n",
    "    plt.scatter(xx,Y[:nPC,i],c=np.array([\"b\", \"r\"])[cornsep[i]],alpha=0.01)\n",
    "plt.xlabel('PC number')\n",
    "plt.ylabel('Value of PC');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that, for some of the principal components, the blue and red dots do not overlap much - this is the case for PC2 for example. If a principal compoment, or a small set of principal components, can be used to differentiate between clean wakes and corner separated wakes, this could be used in an automated classification process.\n",
    "\n",
    "To illustrate this, we plot each wake as a point in a 2D plot where the axes are PC1 and PC2:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(Y[0,:],Y[1,:],c=np.array([\"b\", \"r\"])[cornsep],alpha=0.2)\n",
    "plt.xlabel('PC 1')\n",
    "plt.ylabel('PC 2');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "or in a 3D plot where we use the first 3 principal components as coodinates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(Y[0,:],Y[1,:],Y[2,:],c=np.array([\"b\", \"r\"])[cornsep],alpha=0.2)\n",
    "ax.set_xlabel('PC 1')\n",
    "ax.set_ylabel('PC 2')\n",
    "ax.set_zlabel('PC 3');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The PCA has helped us to see the low dimensional - reduced order - structure that is important in our original, high dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
